{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually generating in-context examples for aligning LLMs with human rating\n",
    "\n",
    "- Get ~100 ratings from an annotator (see `get_relevant_files`)\n",
    "- Select 10 ratings for in-context examples. Ask an LLM (gpt-4o here) to produce reasoning for the rating.\n",
    "- Inject those 10 ratings and their reasonings in the prompt\n",
    "- Now get the LLM to rate the remaining 90 rows\n",
    "- Calculate correlation with human rating using `pearsonr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import csv\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANG = 'ind'\n",
    "SRC_NAME = 'Indonesian'\n",
    "TGT_LANG = 'eng'\n",
    "TGT_NAME = 'English'\n",
    "\n",
    "ANNOTATOR = 'A2'\n",
    "RATINGS_FOR_MODEL = ''\n",
    "\n",
    "RATER_MODEL = 'gemini-1.5-pro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.Google(model=RATER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_relevant_files, extract_rows, remap_columns\n",
    "files = get_relevant_files(SRC_LANG, RATINGS_FOR_MODEL, ANNOTATOR)\n",
    "rows = extract_rows(files)\n",
    "rows = [r for r in rows if r[SRC_LANG]]\n",
    "for r in rows:\n",
    "    remap_columns(r)\n",
    "\n",
    "print(f\"Total of {len(rows)} examples\")\n",
    "print(random.sample(rows, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(rows)\n",
    "# sample are our rows for injecting in context, the rest will be used for evaluation\n",
    "sample = random.sample(rows, 10)\n",
    "\n",
    "print(f'ratings in sample: {[r['Overall rating'] for r in sample]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get LLM reasoning for rating of the rows in `sample`\n",
    "\n",
    "Output: prompt with in-context examples `AUGMENTED_SYSTEM_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = f'''You are assisting in the creation of a bilingual {TGT_NAME}-{SRC_NAME} dictionary.\n",
    "Your task is to rate a candidate sentence pair that illustrates dictionary entries to help linguists select an appropriate example pair.\n",
    "\n",
    "Example sentences should should be:\n",
    "1. Typical: Show typical usage of the word\n",
    "2. Informative: Add value by providing context or additional information\n",
    "3. Intelligible: Be clear, concise, and appropriate for a general audience\n",
    "4. Translation correct: Are sentences a good translation of each other, with fluent grammar and correct usage of words in both languages\n",
    "\n",
    "You are rating the example sentences, not the dictionary entries.\\n\\n'''\n",
    "\n",
    "TEMPLATE_GET_REASONING = \"\"\"<example>\n",
    "Src Entry: {src_entry}\n",
    "Tgt Entry: {tgt_entry}\n",
    "Src Example: {src_example}\n",
    "Tgt Example: {tgt_example}\n",
    "\n",
    "Comment: {comment}\n",
    "Typical: {typical}\n",
    "Informative: {informative}\n",
    "Intelligible: {intelligible}\n",
    "Translation correct: {translation_correct}\n",
    "</example>\n",
    "\n",
    "Reasoning: what is the reasoning for the above ratings? Give your response in one paragraph.\n",
    "\"\"\"\n",
    "\n",
    "def get_templated_row(row):\n",
    "    return TEMPLATE_GET_REASONING.format(\n",
    "        src_entry=row[SRC_LANG],\n",
    "        tgt_entry=row[TGT_LANG],\n",
    "        src_example=row['src_example'],\n",
    "        tgt_example=row['tgt_example'],\n",
    "        comment=row['Comment'],\n",
    "        typical=row['Typical'],\n",
    "        informative=row['Informative'],\n",
    "        intelligible=row['Intelligible'],\n",
    "        translation_correct=row['Translation correct']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "OPENAI_CLIENT = OpenAI()\n",
    "\n",
    "def get_reasoning(row):\n",
    "    prompt = SYSTEM + get_templated_row(row)\n",
    "    return lm(prompt)[0]\n",
    "\n",
    "for row in tqdm(sample, desc=\"Generating reasoning\"):\n",
    "    row['reasoning'] = get_reasoning(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_EXAMPLE = \"\"\"<example>\n",
    "<data>\n",
    "Src Entry: {src_entry}\n",
    "Tgt Entry: {tgt_entry}\n",
    "Src Example: {src_example}\n",
    "Tgt Example: {tgt_example}\n",
    "</data>\n",
    "<reasoning>{reasoning}</reasoning>\n",
    "<rating>{rating}</rating>\n",
    "</example>\"\"\"\n",
    "\n",
    "TEMPLATE_ASK_FOR_RATING = \"\"\"\n",
    "<data>\n",
    "Src Entry: {src_entry}\n",
    "Tgt Entry: {tgt_entry}\n",
    "Src Example: {src_example}\n",
    "Tgt Example: {tgt_example}\n",
    "</data>\"\"\"\n",
    "\n",
    "def get_templated_example(row):\n",
    "    return TEMPLATE_EXAMPLE.format(\n",
    "        src_entry=row[SRC_LANG],\n",
    "        tgt_entry=row[TGT_LANG],\n",
    "        src_example=row['src_example'],\n",
    "        tgt_example=row['tgt_example'],\n",
    "        reasoning=row['reasoning'],\n",
    "        rating=row['Overall rating']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "AUGMENTED_SYSTEM_PROMPT = SYSTEM\n",
    "for row in sample:\n",
    "    AUGMENTED_SYSTEM_PROMPT += get_templated_example(row)\n",
    "    AUGMENTED_SYSTEM_PROMPT += '\\n\\n'\n",
    "\n",
    "print(AUGMENTED_SYSTEM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate the remaining rows using ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def templated_ask_for_rating(row):\n",
    "    return TEMPLATE_ASK_FOR_RATING.format(\n",
    "        src_entry=row[SRC_LANG],\n",
    "        tgt_entry=row[TGT_LANG],\n",
    "        src_example=row['src_example'],\n",
    "        tgt_example=row['tgt_example']\n",
    "    )\n",
    "\n",
    "print(templated_ask_for_rating(rows[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_in_sample(row):\n",
    "    # this will filter out more than 10 rows, which is fine, we don't want to re-use rows that have the same word entries as those in the prompt\n",
    "    for r in sample:\n",
    "        if r[SRC_LANG] == row[SRC_LANG] and r[TGT_LANG] == row[TGT_LANG]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def templated_ask_for_rating(row):\n",
    "    return TEMPLATE_ASK_FOR_RATING.format(\n",
    "        src_entry=row[SRC_LANG],\n",
    "        tgt_entry=row[TGT_LANG],\n",
    "        src_example=row['src_example'],\n",
    "        tgt_example=row['tgt_example']\n",
    "    )\n",
    "\n",
    "def get_rating(row):\n",
    "    prompt = AUGMENTED_SYSTEM_PROMPT + templated_ask_for_rating(row)\n",
    "    return lm(prompt)[0]\n",
    "\n",
    "def extract_rating(text):\n",
    "    # keep the part after '</reasoning>'\n",
    "    text = text.split('</reasoning>')[1].strip()\n",
    "    # keep the part after 'Rating: '\n",
    "    text = text.split('<rating>')[1].strip()\n",
    "    return int(text[0])\n",
    "\n",
    "devset = [r for r in rows if not row_in_sample(r)]\n",
    "\n",
    "for row in tqdm([r for r in devset if not 'pred' in r], desc=\"Rating examples\"):\n",
    "    if 'pred' in row:\n",
    "        continue\n",
    "    row['pred'] = get_rating(row)\n",
    "    row['rating_pred'] = extract_rating(row['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LLM ratings to a file\n",
    "\n",
    "with open(f'rating_pred_{ANNOTATOR}_{RATER_MODEL}_{SRC_LANG}_{TGT_LANG}.tsv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=devset[0].keys(), delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    for row in devset:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved ratings to rating_pred_{ANNOTATOR}_{RATER_MODEL}_{SRC_LANG}_{TGT_LANG}.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(devset[0]['Overall rating'], str):\n",
    "    for row in devset:\n",
    "        row['Overall rating'] = int(row['Overall rating'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_preds(rows):\n",
    "    return [r['rating_pred'] for r in rows]\n",
    "\n",
    "def get_refs(rows):\n",
    "    # return [int(r['Overall rating'][0]) for r in rows]\n",
    "    return [r['Overall rating'] for r in rows]\n",
    "\n",
    "corel, pvalue = pearsonr(get_refs(devset), get_preds(devset))\n",
    "print(f\"Pearson correlation: {corel:.3f}, p-value: {pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(f'rating_pred_{ANNOTATOR}_{RATER_MODEL}_{SRC_LANG}_{TGT_LANG}.tsv') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    devset = list(reader)\n",
    "\n",
    "def convert_rating_int(rows):\n",
    "    for row in rows:\n",
    "        row['Overall rating'] = int(row['Overall rating'][0])\n",
    "        row['rating_pred'] = int(row['rating_pred'][0])\n",
    "\n",
    "if isinstance(devset[0]['Overall rating'], str):\n",
    "    convert_rating_int(devset)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(devset)\n",
    "df_grouped = df.groupby(['Overall rating', 'rating_pred']).size().reset_index(name='counts')\n",
    "\n",
    "plt.scatter(df_grouped['Overall rating'], df_grouped['rating_pred'], s=df_grouped['counts']*100, color='blue', alpha=0.7)\n",
    "\n",
    "plt.xlim(1, 5.5)\n",
    "plt.ylim(1, 5.5)\n",
    "\n",
    "plt.xlabel('Reference')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(f'Scatter plot of Gemini-predicted vs {ANNOTATOR}-reference ratings ({SRC_LANG})')\n",
    "\n",
    "plt.savefig(f'rating_pred_{ANNOTATOR}_{RATER_MODEL}_{SRC_LANG}_{TGT_LANG}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
