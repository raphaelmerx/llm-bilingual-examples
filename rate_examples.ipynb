{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_CLIENT = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_LANG = 'ind'\n",
    "SRC_NAME = 'French'\n",
    "TGT_LANG = 'eng'\n",
    "TGT_NAME = 'English'\n",
    "\n",
    "\n",
    "# ANNOTATOR = 'A1'\n",
    "# RATINGS_FOR_MODEL = 'llama'\n",
    "ANNOTATOR = ''\n",
    "RATINGS_FOR_MODEL = ''\n",
    "\n",
    "metrics = 'Typical\tInformative\tIntelligible\tTranslation correct'\n",
    "metrics = metrics.split('\\t')\n",
    "\n",
    "def remap_keys(row, model):\n",
    "    # remap keys for examples\n",
    "    row['src_example'] = row[f'example.{model}_src']\n",
    "    row['tgt_example'] = row[f'example.{model}_tgt']\n",
    "    # remap SRC_LANG to 'src_word'\n",
    "    row['src_word'] = row[SRC_LANG]\n",
    "    row['tgt_word'] = row[TGT_LANG]\n",
    "    # convert rating to int\n",
    "    row['Overall rating'] = int(row['Overall rating'][0])\n",
    "    row = {k: v for k, v in row.items() if k in ['src_example', 'tgt_example', 'src_word', 'tgt_word', 'Overall rating']}\n",
    "    return row\n",
    "\n",
    "def average_out_ratings(rows):\n",
    "    # fow rows that share the same src_example and tgt_example, average out the ratings\n",
    "    examples_to_ratings = {}\n",
    "    for row in rows:\n",
    "        key = (row['src_example'], row['tgt_example'], row['src_word'], row['tgt_word'])\n",
    "        if key not in examples_to_ratings:\n",
    "            examples_to_ratings[key] = []\n",
    "        examples_to_ratings[key].append(row['Overall rating'])\n",
    "    # average out ratings\n",
    "    for key, ratings in examples_to_ratings.items():\n",
    "        examples_to_ratings[key] = sum(ratings) / len(ratings)\n",
    "    # convert to list\n",
    "    examples_to_ratings = [{'src_example': k[0], 'tgt_example': k[1], 'src_word': k[2], 'tgt_word': k[3], 'Overall rating': v} for k, v in examples_to_ratings.items()]\n",
    "    return examples_to_ratings\n",
    "\n",
    "def get_rated_examples_model(lang, model):\n",
    "    files = f'select_examples_{model}_{lang}_eng_rated_*.tsv'\n",
    "    files = glob.glob(files)\n",
    "    print(f\"Found {len(files)} files\")\n",
    "    rows = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                if not row[SRC_LANG]:\n",
    "                    continue\n",
    "                row = remap_keys(row, model)\n",
    "                rows.append(row)\n",
    "    examples_to_ratings = average_out_ratings(rows)\n",
    "    return examples_to_ratings\n",
    "\n",
    "averaged_rows = get_rated_examples_model(SRC_LANG, 'llama')\n",
    "print(f\"Total of {len(averaged_rows)} examples with averaged ratings\")\n",
    "random.choice(averaged_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_relevant_files, extract_rows\n",
    "\n",
    "def get_model_rows(model):\n",
    "    files = get_relevant_files(SRC_LANG, ratings_for_model=model, annotator='')\n",
    "    rows = extract_rows(files)\n",
    "    for r in rows:\n",
    "        r['Overall rating'] = int(r['Overall rating'][0])\n",
    "    return rows\n",
    "\n",
    "for model in ['llama', 'gpt4']:\n",
    "    rows = get_model_rows(model)\n",
    "    avg = sum(r['Overall rating'] for r in rows) / len(rows)\n",
    "    print(f\"Model {model} has {len(rows)} examples with average rating {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between humans\n",
    "\n",
    "between humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "\n",
    "\n",
    "LANG = f\"{SRC_LANG}_{TGT_LANG}\"\n",
    "ANNOTATORS = ['A1', 'A2']\n",
    "\n",
    "from utils import get_relevant_files, extract_rows\n",
    "\n",
    "def get_annotator_rows(annotator):\n",
    "    files = get_relevant_files(SRC_LANG, ratings_for_model=RATINGS_FOR_MODEL, annotator=annotator)\n",
    "    rows = extract_rows(files)\n",
    "    for r in rows:\n",
    "        r['Overall rating'] = int(r['Overall rating'][0])\n",
    "    return rows\n",
    "\n",
    "rows_1 = get_annotator_rows(ANNOTATORS[0])\n",
    "rating1 = [r['Overall rating'] for r in rows_1]\n",
    "rows_2 = get_annotator_rows(ANNOTATORS[1])\n",
    "rating2 = [r['Overall rating'] for r in rows_2]\n",
    "\n",
    "def print_mean_and_ttest():\n",
    "    print(f\"Average overall rating for {ANNOTATORS[0]}: {sum(rating1) / len(rating1):.2f}\")\n",
    "    print(f\"Standard deviation for {ANNOTATORS[0]}: {pd.Series(rating1).std():.2f}\")\n",
    "    print(f\"Average overall rating for {ANNOTATORS[1]}: {sum(rating2) / len(rating2):.2f}\")\n",
    "    print(f\"Standard deviation for {ANNOTATORS[1]}: {pd.Series(rating2).std():.2f}\")\n",
    "    t_statistic, p_value = ttest_rel(rating1, rating2)\n",
    "    print(f\"t-statistic: {t_statistic:.3f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "print_mean_and_ttest()\n",
    "\n",
    "print(f\"Working with a total of {len(rows_1)} and {len(rows_2)} rows\")\n",
    "\n",
    "def rating_to_bucket(rating: int):\n",
    "    if rating in [1, 2]:\n",
    "        return 'Bad'\n",
    "    if rating == 3:\n",
    "        return 'Neutral'\n",
    "    if rating in [4, 5]:\n",
    "        return 'Good'\n",
    "\n",
    "def calculate_overall_rating_agreement_buckets():\n",
    "    rating1 = [rating_to_bucket(r) for r in rating1]\n",
    "    rating2 = [rating_to_bucket(r) for r in rating2]\n",
    "    return cohen_kappa_score(rating1, rating2)\n",
    "\n",
    "\n",
    "def calculate_overall_rating_agreement():\n",
    "    cohenkappa = cohen_kappa_score(rating1, rating2, weights='quadratic')\n",
    "    print(f\"Cohen's Kappa: {cohenkappa:.3f}\")\n",
    "\n",
    "    # pearsonr\n",
    "    p = pearsonr(rating1, rating2)\n",
    "    print(f\"Pearson's r: {p[0]:.3f}, p-value: {p[1]:.3f}\")\n",
    "\n",
    "calculate_overall_rating_agreement()\n",
    "# TMP: use buckets instead\n",
    "# print(calculate_overall_rating_agreement_buckets(rows_1, rows_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate average over different metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "metrics = 'Typical\tInformative\tIntelligible\tTranslation correct'\n",
    "metrics = metrics.split('\\t')\n",
    "\n",
    "def map_rating_to_int(rating):\n",
    "    if rating == 'Yes':\n",
    "        return 1\n",
    "    if rating == 'No':\n",
    "        return 0\n",
    "    return 0.5\n",
    "\n",
    "def get_values_for_metric_as_list(metric, rows_1, rows_2=None, map_to_int=True):\n",
    "    values = [r[metric] for r in rows_1]\n",
    "    if rows_2:\n",
    "        values.extend([r[metric] for r in rows_2])\n",
    "    # map 'Yes' to 1 and 'No' to 0, 'Somewhat' to 0.5\n",
    "    if map_to_int:\n",
    "        values = [map_rating_to_int(v) for v in values]\n",
    "    values = [v for v in values if v is not None]\n",
    "    return values\n",
    "\n",
    "print(f\"Calculating metrics for model {RATINGS_FOR_MODEL} and lang {LANG}\")\n",
    "print()\n",
    "for metric in metrics:\n",
    "    values = get_values_for_metric_as_list(metric, rows_1, rows_2)\n",
    "    # calculate average\n",
    "    average = sum(values) / len(values)\n",
    "    print(f\"Metric {metric} average: {average:.3f} over {len(values)} ratings\")\n",
    "    # calculate correlation with pearsonr\n",
    "\n",
    "\n",
    "# overall rating: just canculate the avg\n",
    "metric = 'Overall rating'\n",
    "values = get_values_for_metric_as_list(metric, rows_1, rows_2, map_to_int=False)\n",
    "average = sum(values) / len(values)\n",
    "print(f\"Metric {metric} average: {average:.3f} over {len(values)} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "LANG = f\"{SRC_LANG}_{TGT_LANG}\"\n",
    "MODELS = ['llama', 'gpt4']\n",
    "\n",
    "ANNOTATORS = ['A1', 'A2']\n",
    "\n",
    "from utils import get_relevant_files, extract_rows\n",
    "\n",
    "def get_model_rows(lang, model):\n",
    "    files = get_relevant_files(lang, ratings_for_model=model, annotator='')\n",
    "    rows = extract_rows(files)\n",
    "    for r in rows:\n",
    "        r['Overall rating'] = int(r['Overall rating'][0])\n",
    "    return rows\n",
    "\n",
    "def calculate_rating_statistics(model_rows):\n",
    "    for model in ['gpt4', 'llama']:\n",
    "        ratings = [r['Overall rating'] for r in model_rows[model]]\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        mean = np.mean(ratings)\n",
    "        std = np.std(ratings)\n",
    "        \n",
    "        print(f\"\\n{model.upper()} Statistics:\")\n",
    "        print(f\"Mean: {mean:.3f}\")\n",
    "        print(f\"Standard Deviation: {std:.3f}\")\n",
    "\n",
    "def calculate_ttest(model_rows):\n",
    "    gpt4_ratings = [r['Overall rating'] for r in model_rows['gpt4']]\n",
    "    llama_ratings = [r['Overall rating'] for r in model_rows['llama']]\n",
    "\n",
    "    # Paired t-test\n",
    "    t_statistic, p_value = stats.ttest_rel(gpt4_ratings, llama_ratings)\n",
    "    print(f\"\\nPaired t-test: t-statistic = {t_statistic:.4f}, p-value = {p_value:.4f}\")\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    d = (np.mean(gpt4_ratings) - np.mean(llama_ratings)) / np.std(np.array(gpt4_ratings) - np.array(llama_ratings))\n",
    "    print(f\"Cohen's d effect size: {d:.4f}\")\n",
    "\n",
    "for lang in ['fra', 'ind', 'tdt']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analysis for {lang}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    model_rows = {model: get_model_rows(lang, model) for model in MODELS}\n",
    "    calculate_rating_statistics(model_rows)\n",
    "    calculate_ttest(model_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_relevant_files, extract_rows\n",
    "\n",
    "files = get_relevant_files(SRC_LANG, RATINGS_FOR_MODEL, ANNOTATOR)\n",
    "rows = extract_rows(files)\n",
    "\n",
    "values = {}\n",
    "for metric in metrics:\n",
    "    # values can be Yes, Somewhat, No\n",
    "    values[metric] = get_values_for_metric_as_list(metric, rows, rows_2=None, map_to_int=False)\n",
    "\n",
    "# stacked bar plot for each metric\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for stacking\n",
    "yes_counts = [v.count('Yes') for v in values.values()]\n",
    "somewhat_counts = [v.count('Somewhat') for v in values.values()]\n",
    "no_counts = [v.count('No') for v in values.values()]\n",
    "\n",
    "def print_percent_per_metric():\n",
    "    print(SRC_LANG)\n",
    "    for metric, yes, somewhat, no in zip(metrics, yes_counts, somewhat_counts, no_counts):\n",
    "        total = yes + somewhat + no\n",
    "        print(f\"{metric}: {yes_counts} yes, {somewhat_counts} somewhat, {no_counts} no\")\n",
    "        # print(f\"{metric}: {yes/total*100:.2f}% yes, {somewhat/total*100:.2f}% somewhat, {no/total*100:.2f}% no\")\n",
    "print_percent_per_metric()\n",
    "\n",
    "# Set the positions of the bars\n",
    "bar_width = 0.5\n",
    "indices = np.arange(len(metrics))\n",
    "\n",
    "# Plot each stack\n",
    "plt.bar(indices, yes_counts, bar_width, label='Yes', color='lightgreen', edgecolor='black')\n",
    "plt.bar(indices, somewhat_counts, bar_width, bottom=yes_counts, label='Somewhat', color='lightyellow', edgecolor='black')\n",
    "plt.bar(indices, no_counts, bar_width, bottom=np.array(yes_counts) + np.array(somewhat_counts), label='No', color='lightcoral', edgecolor='black')\n",
    "\n",
    "# Labels and title\n",
    "plt.xticks(indices, [metric if metric != \"Translation correct\" else \"Trans. correct\" for metric in metrics], fontsize=18)\n",
    "plt.yticks([0, 50, 100, 150, 200], rotation=90, fontsize=18)  # Show y ticks oriented vertically with specific values\n",
    "# plt.xlabel('Metrics')\n",
    "plt.ylabel('Count')\n",
    "# plt.title(f'{SRC_NAME} metric distribution')\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SRC_LANG} metric distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate correlation over metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import krippendorff\n",
    "\n",
    "\n",
    "metrics = 'Typical\tInformative\tIntelligible\tTranslation correct'\n",
    "metrics = metrics.split('\\t')\n",
    "\n",
    "print(f\"Language: {LANG}\")\n",
    "\n",
    "for metric in metrics:\n",
    "    rows_1_metric = [map_rating_to_int(r[metric]) for r in rows_1]\n",
    "    rows_2_metric = [map_rating_to_int(r[metric]) for r in rows_2]\n",
    "    p = pearsonr(rows_1_metric, rows_2_metric)\n",
    "    k = krippendorff.alpha(reliability_data=[rows_1_metric, rows_2_metric], level_of_measurement='interval')\n",
    "    print(f\"{metric} Pearson's r: {p[0]:.3f}, p-value: {p[1]:.3f}; Krippendorff's alpha: {k:.3f}\")\n",
    "\n",
    "metric = 'Overall rating'\n",
    "rows_1_metric = [r[metric] for r in rows_1]\n",
    "rows_2_metric = [r[metric] for r in rows_2]\n",
    "p = pearsonr(rows_1_metric, rows_2_metric)\n",
    "krippendorff.alpha(reliability_data=[rows_1_metric, rows_2_metric], level_of_measurement='interval')\n",
    "print(f\"{metric} Pearson's r: {p[0]:.3f}, p-value: {p[1]:.3f}; Krippendorff's alpha: {k:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Language: fra_eng\n",
    "Typical Pearson's r: 0.386, p-value: 0.000; Krippendorff's alpha: 0.378\n",
    "Informative Pearson's r: -0.052, p-value: 0.610; Krippendorff's alpha: -0.047\n",
    "Intelligible Pearson's r: 0.263, p-value: 0.008; Krippendorff's alpha: 0.264\n",
    "Translation correct Pearson's r: 0.146, p-value: 0.146; Krippendorff's alpha: 0.136\n",
    "Overall rating Pearson's r: 0.322, p-value: 0.001; Krippendorff's alpha: 0.136\n",
    "\n",
    "Language: ind_eng\n",
    "Typical Pearson's r: 0.518, p-value: 0.000; Krippendorff's alpha: 0.517\n",
    "Informative Pearson's r: 0.187, p-value: 0.062; Krippendorff's alpha: -0.198\n",
    "Intelligible Pearson's r: -0.040, p-value: 0.690; Krippendorff's alpha: -0.036\n",
    "Translation correct Pearson's r: -0.084, p-value: 0.407; Krippendorff's alpha: -0.083\n",
    "Overall rating Pearson's r: 0.071, p-value: 0.482; Krippendorff's alpha: -0.083\n",
    "\n",
    "Language: tdt_eng\n",
    "Typical Pearson's r: 0.585, p-value: 0.000; Krippendorff's alpha: 0.548\n",
    "Informative Pearson's r: 0.479, p-value: 0.000; Krippendorff's alpha: 0.449\n",
    "Intelligible Pearson's r: 0.598, p-value: 0.000; Krippendorff's alpha: 0.519\n",
    "Translation correct Pearson's r: 0.529, p-value: 0.000; Krippendorff's alpha: 0.529\n",
    "Overall rating Pearson's r: 0.573, p-value: 0.000; Krippendorff's alpha: 0.529\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
