{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import csv\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "OPENAI_CLIENT = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_LANG = 'fra'\n",
    "SRC_NAME = 'French'\n",
    "TGT_LANG = 'eng'\n",
    "TGT_NAME = 'English'\n",
    "\n",
    "\n",
    "ANNOTATOR = 'A1'\n",
    "\n",
    "EVAL_ON = 'src'\n",
    "# eval_on = 'tgt'\n",
    "\n",
    "RATINGS_FOR_MODEL = ''\n",
    "# RATINGS_FOR_MODEL = ''\n",
    "\n",
    "# whether 2 ratings for the same sentences will be averaged\n",
    "AVERAGE_OUT_RATINGS = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get annotations\n",
    "\n",
    "And remap columns using standard src_example, tgt_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# { 'src_word': ..., 'tgt_word': ..., 'src_example': ..., 'tgt_example': ..., 'rating': ...}\n",
    "from utils import get_relevant_files, extract_rows, remap_columns\n",
    "\n",
    "files = get_relevant_files(src_lang=SRC_LANG, ratings_for_model=RATINGS_FOR_MODEL, annotator=ANNOTATOR)\n",
    "rows = extract_rows(files, remove_empty=False)\n",
    "rows = [r for r in rows if r['Overall rating']]\n",
    "\n",
    "\n",
    "# for each row, remap column that starts with 'example' and ends with 'src' with 'src_example'\n",
    "# and the same for 'tgt_example'\n",
    "\n",
    "for r in rows:\n",
    "    remap_columns(r)\n",
    "\n",
    "print(f\"Total of {len(rows)} rated examples\")\n",
    "print(random.choice(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tracker = pd.DataFrame(columns=['Language', 'Model', 'Model output', 'correlated with', 'correlation', 'p-value'])\n",
    "if ANNOTATOR:\n",
    "    # add column annotator\n",
    "    results_tracker['Annotator'] = ANNOTATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "MODEL_PATH = \"FacebookAI/xlm-roberta-large\"\n",
    "\n",
    "if SRC_LANG == 'fra':\n",
    "    MODEL_PATH = 'almanach/camembert-large'\n",
    "    # switch to camemberta, based on deberta\n",
    "    # MODEL_PATH = 'almanach/camemberta-base'\n",
    "elif SRC_LANG == 'ind':\n",
    "    MODEL_PATH = \"indolem/indobert-base-uncased\"\n",
    "    # MODEL_PATH = 'LazarusNLP/NusaBERT-large' . # very bad results with this model, none of them significant\n",
    "elif SRC_LANG == 'tdt':\n",
    "    MODEL_PATH = 'raphaelmerx/xlm-roberta-large-tetun'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline \n",
    "import math\n",
    "\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=MODEL_PATH, tokenizer=MODEL_PATH, device=0)\n",
    "\n",
    "def replace_word_with_token(word, text, mask_token):\n",
    "    text = text.lower()\n",
    "    word = word.lower()\n",
    "    if word not in text and SRC_LANG == 'fra':\n",
    "        # some French lemmas\n",
    "        text = text.replace('média', 'media').replace('avalé', 'avaler').replace('tombé', 'tomber').replace('exaucés', 'exaucer')\n",
    "    elif word not in text and SRC_LANG == 'tdt':\n",
    "        text = text.replace(\"ha'u-nia\", \"ha’u-nia\").replace('orgaun', 'órgaun').replace(\"exemplu\", \"examplu\").replace(\"kompaniia\", \"kompañia\")\n",
    "        \n",
    "    text = text.replace(word, mask_token)\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)['input_ids']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, labels=inputs)\n",
    "    \n",
    "    neg_log_likelihood = outputs.loss.item()\n",
    "    perplexity = torch.exp(torch.tensor(neg_log_likelihood)).item()\n",
    "    return perplexity\n",
    "\n",
    "def calculate_masked_word_prob(word, text):\n",
    "    text = replace_word_with_token(word, text, tokenizer.mask_token)\n",
    "    try:\n",
    "        results = fill_mask(text)\n",
    "    except:\n",
    "        print(f\"Error for text: {text} and word: {word}\")\n",
    "        return 0\n",
    "    # can be more than 1 result, if the word appears multiple times\n",
    "    # if mask appears multiple times, take the first one\n",
    "    if text.count(tokenizer.mask_token) > 1:\n",
    "        results = results[0]\n",
    "    return sum(r['score'] for r in results if r['token_str'] == word)\n",
    "\n",
    "def calculate_entropy(word, text):\n",
    "    text = replace_word_with_token(word, text, tokenizer.mask_token)\n",
    "    try:\n",
    "        results = fill_mask(text)\n",
    "    except:\n",
    "        print(f\"Error for text: {text}\")\n",
    "        return 0\n",
    "    \n",
    "    if text.count(tokenizer.mask_token) > 1:\n",
    "        results = results[0]\n",
    "    entropy = -sum(r['score'] * math.log(r['score'], 2) for r in results)\n",
    "    return entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def convert_to_float(value):\n",
    "    if value == \"Yes\":\n",
    "        return 1\n",
    "    if value == \"No\":\n",
    "        return 0\n",
    "    return 0.5\n",
    "\n",
    "metrics = ['Typical', 'Informative', 'Intelligible', 'Translation correct', 'rating']\n",
    "\n",
    "sentences_with_rating = [\n",
    "    {\n",
    "        'word': r[SRC_LANG if EVAL_ON == 'src' else TGT_LANG],\n",
    "        'rating': int(r['Overall rating'][0]),\n",
    "        'text': r['src_example' if EVAL_ON == 'src' else 'tgt_example'],\n",
    "        'Typical': convert_to_float(r['Typical']),\n",
    "        'Informative': convert_to_float(r['Informative']),\n",
    "        'Intelligible': convert_to_float(r['Intelligible']),\n",
    "        'Translation correct': convert_to_float(r['Translation correct'])\n",
    "    } for r in rows\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_perplexity(sentences_with_rating):\n",
    "    for s in tqdm(sentences_with_rating, desc='Calculating perplexity'):\n",
    "        s['perplexity'] = calculate_perplexity(s['text'])\n",
    "        s['word_perplexity'] = calculate_perplexity(s['word'])\n",
    "        s['weighted_perplexity'] = s['perplexity'] / s['word_perplexity']\n",
    "\n",
    "def add_masked_probabilities(sentences_with_rating):\n",
    "    for s in tqdm(sentences_with_rating, desc='Calculating masked probabilities'):\n",
    "        s['word_probability'] = calculate_masked_word_prob(s['word'], s['text'])\n",
    "\n",
    "def add_entropy(sentences_with_rating):\n",
    "    for s in tqdm(sentences_with_rating, desc='Calculating entropy'):\n",
    "        s['entropy'] = calculate_entropy(s['word'], s['text'])\n",
    "\n",
    "add_masked_probabilities(sentences_with_rating)\n",
    "add_entropy(sentences_with_rating)\n",
    "add_perplexity(sentences_with_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.DataFrame(sentences_with_rating)\n",
    "\n",
    "def get_corel_perplexity(metric = 'rating'):\n",
    "    correlation, p_value = pearsonr(data['perplexity'], data[metric])\n",
    "    row = [SRC_LANG, MODEL_PATH, 'Perplexity', metric, correlation, p_value]\n",
    "    if ANNOTATOR:\n",
    "        row.append(ANNOTATOR)\n",
    "    results_tracker.loc[len(results_tracker)] = row\n",
    "    print(f\"Perplexity correlation for {metric}: {correlation:.3f}, p-value: {p_value}\")\n",
    "\n",
    "def get_corel_word_prob(metric = 'Informative'):\n",
    "    correlation, p_value = pearsonr(data['word_probability'], data[metric])\n",
    "    row = [SRC_LANG, MODEL_PATH, 'Masked LM', metric, correlation, p_value]\n",
    "    if ANNOTATOR:\n",
    "        row.append(ANNOTATOR)\n",
    "    results_tracker.loc[len(results_tracker)] = row\n",
    "    print(f\"Masked LM correlation for {metric}: {correlation:.3f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "def get_corel_entropy(metric = 'rating'):\n",
    "    correlation, p_value = pearsonr(data['entropy'], data[metric])\n",
    "    row = [SRC_LANG, MODEL_PATH, 'Entropy', metric, correlation, p_value]\n",
    "    if ANNOTATOR:\n",
    "        row.append(ANNOTATOR)\n",
    "    results_tracker.loc[len(results_tracker)] = row\n",
    "    print(f\"Entropy correlation for {metric}: {correlation:.3f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "print(f\"Working with language {SRC_LANG} and model {MODEL_PATH}\")\n",
    "print(f\"Annotator: {ANNOTATOR}, model: {RATINGS_FOR_MODEL}\")\n",
    "print()\n",
    "for metric in metrics:\n",
    "    get_corel_word_prob(metric)\n",
    "    get_corel_perplexity(metric)\n",
    "    get_corel_entropy(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANNOTATOR:\n",
    "    filename = f'{SRC_LANG}_{ANNOTATOR}_results_tracker.csv'\n",
    "else:\n",
    "    filename = f'{SRC_LANG}_results_tracker.csv'\n",
    "\n",
    "results_tracker.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['fra', 'ind', 'tdt']\n",
    "annotator = 'A1'\n",
    "\n",
    "allfiles = []\n",
    "\n",
    "for lang in langs:\n",
    "    files = glob(f'{lang}_{annotator}_results_tracker.csv')\n",
    "    if files:\n",
    "        allfiles.extend(files)\n",
    "\n",
    "print(allfiles)\n",
    "# dataframe with all results\n",
    "df = pd.concat([pd.read_csv(f) for f in allfiles])\n",
    "df.to_csv('all_results_tracker.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
